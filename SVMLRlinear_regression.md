# SVM、logistic、linear regression

* 线性回归为什么不能做分类任务？

  线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；LR和SVM克服了这一点，其中LR将所有数据采用sigmoid函数进行了非线性映射；SVM直接去掉了原理分类决策面的数据，只考虑支持向量的影响。

* 有异常点的时候，用LR还是SVM？
  在线性分类的时候，如果异常点较多无法剔除，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献；SVM软件隔对异常比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料

## 联系

* LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题
* 两个方法都可以增加不同的正则化项
* LR和SVM都可以用来做非线性分类，只要加核函数就好
* LR和SVM都是线形模型，当然这里不考虑核函数
* 都属于判别模型



## 区别

* LR是参数模型，SVM是非参数模型
* 逻辑回归用的是logistical loss， SVM采用的是hinge loss
* 逻辑回归相对来说模型更简单，SVM的理解和优化来说更复杂一些，SVM在转换成对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算
* SVM不直接依赖数据分布，而LR则依赖，因为SVM只与支持向量那几个点有关系，而LR和所有的点都有关系。
* SVM依赖penalty系数，实验中需要做CV
* SVM本身是结构风险最小化模型，而LR是经验风险最小化模型

### SVM

#### 优点

* 凸优化问题，求得解一定是全局最优
* 不仅适用于线性问题，还适用于非线形问题
* 拥有高维样本空间的数据也能SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，避免“维度灾难”
* 理论基础比较完善

#### 缺点

* 二次规划问题求解涉及M阶矩阵的计算，因此svm不适用于超大数据集
* 只适用于二分类问题（可以通过多个SVM的组合解决多分类问题）