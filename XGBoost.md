* xgboost 是boosting算法中的一种，Boosting算法将许多弱分类器集成在一起形成一个强分类器。

* XGboost是一种提升树模型，所以他是将许多树模型集成在一起，形成一个很强的分类器。

* 用到的树模型是CART树模型：
  * CART分类树用的是基尼指数（Gini)
    $Gini(D)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$
    每次选取gini指数最小的变量与值进行分割，而对于样本集合D根据特征A是否可能取某一可能值a被分割为D1和D2，则在特征A的前提下，集合D的基尼指数定义为
    $$
    Gini(D,A)={|D_1|\over D}Gini(D_1)+{|D_2|\over D}Gini(D_2)
    $$
    可以看出，样本不确定性越大，基尼指数越大.
    
    1. 在所有可能特征A及它们所有可能的切分点a中，选取基尼指数最小的特征及其切分点作为最优特征及最优切分点，由此分割D，从现结点生成两个子结点，分配训练数据集.
    2. 递归调用，直到条件停止。
    3. 生成决策树。
    
    
    
  * CART回归树

    递归地构建二叉决策树，对回归树用平方误差最小化准则。

    1. 选择最优切分变量j与切分点求s解

    2. 用选定的对（j,s)划分区域并决定相应的输出值：
       $$
       R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}\ge s\}\\
       \bar{c}_m={1\over N}\sum_{x_i\in R_m(j,s)}y_i, x_i\in R_m, m=1,2\\
       min[min_{c_1} \sum_{x\in R_1(j,s)}(y_i-c_1)^2+min_{c2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
       $$
       遍历j（第j个特征）,对固定j扫面s（第j个特征对应的值）。

    3. 重复上述步骤至条件停止

    4. 将输入空间划分为M个区域，生成决策树,以M个区域的均值作为输出。

* Boosting Tree
  * 初始化 $f_0(x)=0$
  * 对 $m=1,2,\cdots,M$
  * 计算残差，$r_{mi}=y_i-F_{m-1}(x_i), i=1,2,3,\cdots,N$
  * 拟合残差 $r_{mi}$,学习一个回归树，得到$f_m(x)$
  * 更新 $f_m(x)=F_{m-1}(x)+f_m(x)$
  * 得到回归问题提升树 $F_M(x)=\sum_i^Mf_i(x)$ 

* GBDT(Gradient Boosting Decision Tree)
  * 因为我们的目标是最小化$L(y,F(x))$, 所以我们只需要$L(y,\sum_{j=1}^t f_j(x)+f_{t+1}(x)$的值比$L(y,\sum_{j=1}^t f_j(x))$小就好了。

  * 初始化 $f_0(x)=argmin_c\sum_{i=1}^NL(y_i,c)$

  * 对 $m=1,2,\cdots,M$

  * 对 $i=1,2,\cdots,N$, 计算 $r_{mi}=-[{\partial L(y_i,f(x_i))\over \partial f(x_i)}]_{f(x)=F_{m-1}(x)}$

  * 拟合残差 $r_{mi}$ 学习一个cart回归树，得到$f_m(x)$
    $$
    这里以MSE为例\\
    学习第t棵树：w^*=argmin_w\sum_{i=1}^N(r_{mi}-h_t(x_i;w))^2\\
    min[min_{c_1} \sum_{x_i\in R_1(j,s)}(r_{mi}-c_1)^2+min_{c2}\sum_{x_i\in R_2(j,s)}(r_{mi}-c_2)^2]\\
    \text{当选择MSE作为loss function时：} c_{jm}=avg_{x_i\in R_{jm}}r_{mi}\\
    h_t(x_i,w)=c_t\\
    \text{line search 找步长}:p^*=argmin_p\sum_{i=1}^NL(y_i,F_{t-1}(x_i)+ph_t(x_i;w^*))\\
    f_m=p^*h_t(x;w^*)
    $$

  * 更新 $F_m(x)=F_{m-1}(x)+f_m(x)$

  * 得到回归问题提升树 $F_M(x)=\sum_{i=0}^M f_i(x)$

  * 和梯度下降是一个思想

  * 表述：**它是一种基于boosting增强策略的加法模型**，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差，然后这个残差是由一阶导数估计的。

* XGB

  1. 目标函数由两部分组成
     $$
     Obj=\sum_{i=1}^m l(y_i,\bar{y}_i)+\sum_{k=1}^K\Omega{(f_k)}
     $$

     1. XGB是一个加法模型，假设我们要第t次迭代训练的模型是$f_t()$,则有
        $$
        \bar{y_i}^{(t)}=\sum_{k=1}^t f_k(x_i)=\bar{y_i}^{(t-1)}+f_t(x_i)
        $$

     2. 将上式代入Obj：
        $$
        Obj=\sum_{i=1}^m l(y_i,\bar{y}_i)+\sum_{k=1}^K\Omega{(f_k)}\\
        =\sum_{i=1}^nl(y_i,\bar{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+constant
        $$

     3.  由泰勒公式展开，损失函数可转化为下式
        $$
        l(y_i,\bar{y}_i^{(t-1)}+f_t(x_i))=l(y_i,\bar{y}_i^{(t-1)})+g_if_t(x_i)+{1\over2}h_if_t^2(x_i)
        $$

     4. 目标函数等于
        $$
        Obj^{(t)}\simeq \sum_{i=1}^n[g_if_t(x_i)+{1\over2}h_if_t^2(x_i)]+\Omega(f_t)\\
        =\sum_{i=1}^n[g_iw_q(x_i)+{1\over2}h_iw_q^2(x_i)]+\lambda T+\lambda {1\over2} \sum_{j=1}^T \omega_j^2\\
        \sum_{j=1}^T[\sum_{i\in I_j}g_iw_j+{1\over2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T\\
        \sum_{j=1}^T[G_jw_j+{1\over2}(H_j+\lambda)w_j^2]+\gamma T\\
        其中，g_i=\delta_{\bar{y}^{(t-1)}}l(y_i,\bar{y}^{(t-1)}),h_i=\delta^2_{\bar{y}^{(t-1)}}l(y_i,\bar{y}^{(t-1)})
        $$

     5. 对Obj求导后，得到使obj最小值$w_j$(叶子结点的输出)
        $$
        w_j^*=-{G_j\over{H_j+\lambda}}\\
        Obj=-{1\over2}\sum_{j=1}^T{G_j^2\over{H_j+\lambda}}+\gamma T
        $$

     6. 增益
        $$
        Gain=Obj_{L+R}-(Obj_L+Obj_R)\\
        =[-{1\over2}{(G_L+G_R)^2\over{H_L+H_R+\lambda}}+\gamma]-[-{1\over2}({G_L^2\over{H_L+\lambda}}+{G_R^2\over{H_R+\lambda}})+2\gamma]
        $$
        找到使增益最大的分裂点。

     7. 寻找最佳分类点：

        * 遍历每个结点的每个特征；
        * 对每个特征，按特征值大小将特征值排序；
        * 线性扫描，找出每个特征的最佳分裂特征值；
        * 在所有特征中找出最好的分裂点（分裂后增益最大的特征及特征值

     8. 上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。

     9. 解决方案：

        * **特征预排序+缓存：**XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。
        * **分位点近似法**：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。
        * **并行查找：**由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。

     10.  停止生长

         * 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂.
         * 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。
         * 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。超参数：**最小样本权重和**，防止过拟合。

     11.  表述：XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

* XGBoost与传统GBDT的算法层面的区别
  * **导数信息**：XGBoost中的导数不是一阶的，是二阶的（牛顿method）
  * **正则项**：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
  * **列抽样**：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
  * **缺失值处理**：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。
  * **并行化**：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
  
* XGB防止过拟合的方法：

  1. **目标函数添加正则项**：叶子节点个数+叶子节点权重的L2正则化
  2. **列抽样**：训练的时候只用一部分特征（不考虑剩余的block块即可）
  3. **子采样**：每轮计算可以不使用全部样本，使算法更加保守
  4. **shrinkage**: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间

* XGB如何处理缺失值

  1. 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。
  2. 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
  3. 在预测时，会自动将缺失值的划分方向放到右子结点。

* RF和GBDT的区别

  **相同点**：都是由多棵树组成，最终的结果都是由多棵树一起决定。

  **不同点**：

  1. **集成学习**：RF属于bagging思想，而GBDT是boosting思想
  2. **偏差-方差权衡**：F不断的降低模型的方差，而GBDT不断的降低模型的偏差
  3. **训练样本**：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
  4. **并行性**：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)
  5. **最终结果**：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合
  6. **数据敏感性**：RF对异常值不敏感，而GBDT对异常值比较敏感
  7. **泛化能力**：RF不易过拟合，而GBDT容易过拟合

* 比较LR和GBDT

  1. LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
  2. GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。

* XGB评价特征的重要性

  1. **weight** ：该特征在所有树中被用作分割样本的特征的总次数。
  2. **gain** ：该特征在其出现过的所有树中产生的平均增益
  3. **cover** ：该特征在其出现过的所有树中的平均覆盖范围。

* 参数

  1. **确定learning rate和estimator的数量**：learning rate可以先用0.1，用cv来寻找最优的estimators

  2. **max_depth和 min_child_weight**：

     max_depth，每棵子树的最大深度

     min_child_weight，子节点的权重阈值

  3. **gamma**

     - 如果大于该阈值，则该叶子节点值得继续划分
     - 如果小于该阈值，则该叶子节点不值得继续划分

  4. **subsample, colsample_bytree**

     - subsample是对训练的采样比例
     - colsample_bytree是对特征的采样比例
     - both check from 0.6 to 0.9

  5. **正则化参数**

     - alpha 是L1正则化系数，try 1e-5, 1e-2, 0.1, 1, 100
     - lambda 是L2正则化系数

  6. **降低学习率**

     降低学习率的同时增加树的数量，通常最后设置学习率为0.01~0.1

  

  

  

  

* Bagging,boosting,stacking 之间的区别
  * bagging：
  * 减少方差，对多个估计进行平均。例如：随即森林
  * 对多个模型进行平行训练：一般来说每个模型选取部分数据，部分特征进行训练（对分布有一定改变，增加了泛化能力）
  * 对回归问题，一般是几个模型结果取平均，而分类问题是投票
  * boosting：
  * 减少偏差
  * 通过算法集合将若学习器转换为强学习器的算法
  * 先初始化一个基学习器
  * 再根据基学习器对训练分布样本进行调整
  * 在基于调整后的样本分布来训练下一个基学习器
  * 重复上述步骤
  * e.g. Adaboost, GBDT,XGBoost